from typing import Dict, List
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain.agents import create_agent
import requests
from bs4 import BeautifulSoup
import html2text
import re
import json
from urllib.parse import urljoin
from langchain.tools import tool
load_dotenv()



class WebScraperTools:
    """Collection of web scraping tools."""

    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                        "AppleWebKit/537.36 (KHTML, like Gecko) "
                        "Chrome/122.0.0.0 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
            "Accept-Language": "pl-PL,pl;q=0.9,en-US;q=0.8,en;q=0.7",
            "Connection": "keep-alive",
            "Upgrade-Insecure-Requests": "1",
        })
        self.html_converter = html2text.HTML2Text()
        self.html_converter.ignore_links = False
        self.html_converter.ignore_images = True
        self.scraped_data = {}

    def fetch_page(self, url: str) -> str:
        """Fetch and clean webpage content."""
        try:
            if url in self.scraped_data:
                return f"[Cached]\n{self.scraped_data[url]}"

            response = self.session.get(url, timeout=10, allow_redirects=True)
            response.raise_for_status()

            soup = BeautifulSoup(response.content, "lxml")

            for tag in soup(["script", "style", "nav", "footer", "header"]):
                tag.decompose()

            text = self.html_converter.handle(str(soup))
            text = re.sub(r"\n\s*\n", "\n\n", text).strip()

            self.scraped_data[url] = text
            return text[:10000]

        except Exception as e:
            return f"Error fetching page: {str(e)}"

    def extract_links(self, url: str) -> str:
        """Extract links from page."""
        try:
            response = self.session.get(url, timeout=10)
            response.raise_for_status()

            soup = BeautifulSoup(response.content, "lxml")
            links = []

            for link in soup.find_all("a", href=True):
                href = link["href"]
                text = link.get_text(strip=True)

                if href.startswith("/"):
                    href = urljoin(url, href)

                if href.startswith("http"):
                    links.append({"text": text, "url": href})

            return json.dumps(links[:20], indent=2)

        except Exception as e:
            return f"Error extracting links: {str(e)}"

    def search_web(self, query: str) -> str:
        """Simple DuckDuckGo HTML search."""
        try:
            search_url = f"https://html.duckduckgo.com/html/?q={requests.utils.quote(query)}"
            response = self.session.get(search_url, timeout=10)

            soup = BeautifulSoup(response.content, "lxml")
            results = []

            for result in soup.find_all("div", class_="result")[:5]:
                title_elem = result.find("a", class_="result__a")
                snippet_elem = result.find("a", class_="result__snippet")

                if title_elem:
                    title = title_elem.get_text(strip=True)
                    link = title_elem["href"]
                    snippet = snippet_elem.get_text(strip=True) if snippet_elem else ""

                    results.append(
                        f"TITLE: {title}\nLINK: {link}\nSNIPPET: {snippet}\n"
                    )

            return "\n".join(results) if results else "No results found"

        except Exception as e:
            return f"Error searching: {str(e)}"


class WebScraperAgent:
    """AI Agent for intelligent web scraping."""

    def __init__(self):
        self.scraper_tools = WebScraperTools()

        self.llm = ChatOpenAI(
            model="gpt-4o-mini", 
            temperature=0
        )

        self.agent = self._create_agent()

    def _create_agent(self):

        @tool
        def fetch_webpage(url: str) -> str:
            """Fetch webpage content. Input must be full URL."""
            return self.scraper_tools.fetch_page(url)

        @tool
        def extract_links(url: str) -> str:
            """Extract all links from webpage. Input must be full URL."""
            return self.scraper_tools.extract_links(url)

        @tool
        def search_web(query: str) -> str:
            """Search the web. Input must be search query."""
            return self.scraper_tools.search_web(query)

        tools = [fetch_webpage, extract_links, search_web]

        agent = create_agent(
            model=self.llm,
            tools=tools,
            system_prompt=(
                "You are an intelligent web scraping assistant. "
                "When user provides a URL and asks to extract links, "
                "always use extract_links tool."
            ),
        )

        return agent


    def run(self, query: str) -> str:
        try:
            result = self.agent.invoke(
                {"messages": [{"role": "user", "content": query}]}
            )
            return result["messages"][-1].content

        except Exception as e:
            return f"Error: {str(e)}"


def interactive_scraper():
    agent = WebScraperAgent()
    print("Type 'exit' or 'quit' to stop.\n")

    while True:
        try:
            user_input = input("You: ").strip()
        except KeyboardInterrupt:
            break

        if not user_input:
            continue

        if user_input.lower() in {"exit", "quit"}:
            break

        print("\nAgent thinking...\n")
        response = agent.run(user_input)
        print(f"{response}\n")


if __name__ == "__main__":
    interactive_scraper()
